\documentclass[12pt,oneside]{book} % for one-sided printing

\usepackage{blindtext}% Just used so we can generate some example text
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[export]{adjustbox}
\usepackage{lipsum}
\usepackage{booktabs}  % For better quality tables
\usepackage{tabularx}  % for the X column type
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{xfrac}
\usepackage{algorithmic}
\usepackage{indentfirst}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% Place style file after other packages.
\usepackage{cranfieldthesis}
\usepackage{lscape} % for landscape pages
\usepackage{float}
\usepackage[toc,title,page]{appendix}

% Couleurs personnalisées
\definecolor{backcolour}{rgb}{0.96, 0.96, 0.96} % Fond très clair
\definecolor{codegray}{rgb}{0.47, 0.47, 0.47}   % Commentaires et numéros de ligne
\definecolor{codegreen}{rgb}{0.25, 0.50, 0.35}  % Commentaires
\definecolor{codeblue}{rgb}{0.26, 0.44, 0.58}   % Mots-clés
\definecolor{codepurple}{rgb}{0.50, 0, 0.50}    % Identificateurs
\definecolor{codeteal}{rgb}{0, 0.5, 0.5}        % Chaînes de caractères
\definecolor{terminalback}{rgb}{0.05, 0.05, 0.05} % Fond très sombre pour le terminal
\definecolor{terminaltext}{rgb}{0.7, 0.7, 0.7}    % Texte clair pour le terminal
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{terminalbgcolor}{HTML}{330033}
\definecolor{terminalrulecolor}{HTML}{000099}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codeteal},
    identifierstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{codegray},
    framexleftmargin=15pt,
    framextopmargin=5pt,
    framexbottommargin=5pt,
    framexrightmargin=15pt,
}

\lstdefinestyle{bashstyle}{
    language=bash,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    identifierstyle=\color{codepurple},
    commentstyle=\color{codegreen},
    morecomment=[l]{\#},   % Define comment style
    frame=single,          % adds a frame around the code
    rulecolor=\color{gray},% if not set, the frame-color may be changed on line-breaks
    breakatwhitespace=false,
    breaklines=true,       % sets automatic line breaking
    captionpos=b,          % sets the caption-position to bottom
    keepspaces=true,       % keeps spaces in text
    showspaces=false,      % show spaces everywhere adding particular underscores
    showstringspaces=false % underline spaces within strings only
}

\lstdefinestyle{ymlstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    identifierstyle=\color{codepurple},
    commentstyle=\color{codegreen},
    morecomment=[l]{\#},   % Define comment style
    frame=single,          % adds a frame around the code
    rulecolor=\color{gray},% if not set, the frame-color may be changed on line-breaks
    breakatwhitespace=false,
    breaklines=true,       % sets automatic line breaking
    captionpos=b,          % sets the caption-position to bottom
    keepspaces=true,       % keeps spaces in text
    showspaces=false,      % show spaces everywhere adding particular underscores
    showstringspaces=false, % underline spaces within strings only
    literate=% Special handling for dot notations
        {http.response_time.p99}{{\textcolor{blue}{http.response\_time.p99}}}{18}
        {http.response_time.p95}{{\textcolor{blue}{http.response\_time.p95}}}{18}
        {metrics-by-endpoint}{{\textcolor{blue}{metrics-by-endpoint}}}{16},
    morekeywords={config, flow, target, phases, plugins, scenarios, loop, get, url, count, ensure, apdex, threshold, thresholds, duration, arrivalRate, name, pause, metrics -by-endpoint}, % Additional YAML-specific keywords
}

\lstdefinestyle{servicestyle}{
backgroundcolor=\color{backcolour},      % choose the background color
basicstyle=\ttfamily\scriptsize, % print whole listing white
keywordstyle=\color{blue},          % attributes in blue
stringstyle=\color{red},            % values in red
identifierstyle=\color{codepurple},
breakatwhitespace=false,
breaklines=true,                    % sets automatic line breaking
captionpos=b,                       % sets the caption-position to bottom
keepspaces=true,                    % keeps spaces in text
showspaces=false,                   % show spaces everywhere adding particular underscores
showstringspaces=false,             % underline spaces within strings only
frame=single,                       % adds a frame around the code
rulecolor=\color{gray},             % if not set, the frame-color may be changed on line-breaks
morecomment=[l]{\#},                % Define comment style
moredelim=[s][\color{codegreen}]{[}{]}, % color everything between [ and ] in green
morekeywords={Description,Wants,After,ExecStart,WantedBy,EnvironmentFile,User,Group,Type,Restart,Documentation,WorkingDirectory,RuntimeDirectory,RuntimeDirectoryMode,LimitNOFILE,TimeoutStopSec,CapabilityBoundingSet,DeviceAllow,LockPersonality,MemoryDenyWriteExecute,NoNewPrivileges,PrivateDevices,PrivateTmp,ProtectClock,ProtectControlGroups,ProtectHome,ProtectHostname,ProtectKernelLogs,ProtectKernelModules,ProtectKernelTunables,ProtectProc,ProtectSystem,RemoveIPC,RestrictAddressFamilies,RestrictNamespaces,RestrictRealtime,RestrictSUIDSGID,SystemCallArchitectures,UMask} % add your attributes here
}

% Title Page Set Up
\title{Cloud Computing Assignment}
\author{Alexis Balayre}
\date{2\textsuperscript{nd} January 2024}
\school{\SATM}
\degree{MSc}
\course{Computational Software of Techniques Engineering}
\academicyear{2023 - 2024}

% Supervisors
\supervisor{Dr Stuart Barnes}

% Copyright
\copyrightyear{2024}

% References
\usepackage[numbers]{natbib} % for nice referencing
\makeatletter % Reference list option change to number and period
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %
\setcitestyle{round} % use round citations

\begin{document}

\frontmatter

% Form Title Pages
\maketitle

% Use single spacing for Table of Contents, List of Figures, etc
{
    \clearpage
    \singlespacing
    % Table of Contents
    {
        \tableofcontents
    }
    \clearpage

    % List of Figures
    \listoffigures

    % List of Tables
    \listoftables
}

%% Main Matter
\mainmatter
\pagestyle{fancy}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{\nouppercase{\rightmark}}

\chapter{Introduction}
This report investigates the integration of cloud computing and Internet of
Things (IoT) technologies for real-time environmental monitoring, with a
particular focus on air quality. The main objective of the project is to
develop a robust system capable of efficiently managing large volumes of data
from IoT environmental sensors and accurately calculating the Air Quality Index
(AQI) in real time. This integration is essential for stakeholders such as
researchers, policy makers and the public, who need rapid access to
environmental data to make informed decisions.

The report describes the methodology, system architecture and implementation
strategy adopted to ensure rapid data processing and distribution. Challenges
encountered and corresponding solutions are also discussed, as well as the
wider implications of the system for public health, urban planning and
environmental awareness. This study demonstrates the practical application and
benefits of combining cloud computing and IoT in the management and analysis of
environmental data.

\chapter{Methodologies}\label{chap:one}

\section{Architecture of the Project}
The project is divided into two main pipelines:
\begin{enumerate}
    \item \textbf{Data Collecting, Processing \& Storing Pipeline:} This pipeline is responsible for collecting, processing and storing the data from the IoT sensors in a time-series database.
    \item \textbf{Data Distributing Pipeline:} This pipeline is responsible for distributing the data to the users through a Grafana dashboard and for monitoring the system.
\end{enumerate}
The figure~\ref{fig:project-pipeline} below illustrates the project pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/pipeline.png}
    \caption{Project Architecture Diagram}\label{fig:project-pipeline}
\end{figure}

\newpage
\section{Data Collecting, Processing \& Storing}
\subsection{Overview of the first pipeline architecture}

In the \textbf{Data Collecting} phase, the last set of data is obtained from
its source, marking the start of the data pipeline. Following this, the
\textbf{Data Processing} phase takes over, where the data undergoes formatting
and calculation of the Air Quality Index (AQI) for each particulate matter
sensor. The final phase, \textbf{Data Storing}, involves meticulous storage of
data from each sensor in a time-series database.

For a detailed visual representation of this pipeline, refer to
Figure~\ref{fig:data-collecting-processing-storing-pipeline}, which illustrates
the Data Collecting, Processing, and Storing Pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/cloud-computing-data-ingestion.png}
    \caption{Data Collecting, Processing \& Storing Pipeline Diagram}\label{fig:data-collecting-processing-storing-pipeline}
\end{figure}

\subsection{Data Collecting}
\subsubsection{Data Source}
The Sensor Community network is a global, contributor-driven initiative that
collects open environmental data through a vast network of sensors. These
sensors, deployed in over 70 countries, collect real-time data on air quality,
temperature, humidity and pressure. On average, the sensors send new data every
145 seconds~\cite{sensorcommunity2023synchronization}. The Sensor Community
network offers two main API endpoints for accessing their environmental data:

\begin{enumerate}
    \item \textbf{5-Minute Averaged Data API:} This API provides data averaged over the last 5 minutes for each sensor. This is useful for near real-time analysis or immediate air quality assessments, particularly in test or active monitoring contexts.
    \item \textbf{24 Hour Averaged Data API:} This API provides data averaged over the last 24 hours for each sensor. It is particularly suited to analysing daily trends and understanding environmental changes over a longer period.
\end{enumerate}

\subsubsection{Ingestion Script}
The data acquisition process in this project involved leveraging both APIs from
the Sensor Community. To do this, the Python \texttt{Requests} library was
used, enabling efficient data retrieval. Once collected, the data is
temporarily stored in a local cache, formatted as a Spark DataFrame for
optimised handling and processing. This operation, executed by the
\texttt{fetch\_sensors\_data} function, is located within the
\texttt{collecting.py} script (see Appendix~\ref{appendix:collecting} for
detailed code). In particular, this function is programmed to run at a constant
interval of 10 seconds, ensuring a regular and up-to-date flow of data from the
sensors. This systematic approach not only streamlines the data collection
process, but also improves the reliability and timeliness of the information
collected.

\subsection{Data Processing}

\subsubsection{Processing Script}
The data processing stage initiates with the activation of the
\texttt{computeAQI} function, following data ingestion. This crucial function
uses user-defined functions (UDFs) to convert PM2.5 and PM10 concentration data
into Air Quality Index (AQI) values, adhering to the guidelines specified in
Table~\ref{tab:uk_aqi}. The comprehensive implementation details of this
function are available in the \texttt{processing.py} script, as detailed in
Appendix~\ref{appendix:processing}.

Key to this stage is the utilisation of Spark DataFrames, which offer an
efficient means of managing and processing the data due to their distributed
structure. The data is first restructured using the \texttt{explode} operation,
which separates each sensor's data into individual rows, thus preparing it for
the application of UDFs. To enhance efficiency, the DataFrame is then cached,
significantly reducing the need for repetitive disk read/write operations and
thereby accelerating the computation process.

\begin{table}[H]
    \centering
    \caption{UK air quality index, ``Review of the UK Air Quality Index'', 2011}
    \begin{tabular}{|p{2cm}|p{2cm}|p{3,5cm}|p{3,5cm}|}
        \hline
        \textbf{Range} & \textbf{Air Quality Index} & \textbf{PM\(_{2.5}\) Particles, 24 hour mean (\(\mu g/m^3\))} & \textbf{PM\(_{10}\) Particles, 24 hour mean (\(\mu g/m^3\))} \\ \hline
        Low            & 1                          & 0-11                                                          & 0-16                                                         \\
                       & 2                          & 12-23                                                         & 17-33                                                        \\
                       & 3                          & 24-35                                                         & 34-50                                                        \\ \hline
        Medium         & 4                          & 36-41                                                         & 51-58                                                        \\
                       & 5                          & 42-47                                                         & 59-66                                                        \\
                       & 6                          & 48-53                                                         & 67-75                                                        \\ \hline
        High           & 7                          & 54-58                                                         & 76-83                                                        \\
                       & 8                          & 59-64                                                         & 84-91                                                        \\
                       & 9                          & 65-70                                                         & 92-100                                                       \\ \hline
        Very High      & 10                         & $>$70                                                         & $>$100                                                       \\ \hline
    \end{tabular}
    \label{tab:uk_aqi}
\end{table}

\subsubsection{Leveraging Apache Spark}
The incorporation of Apache Spark is central to the data processing
methodology. Its distributed computing framework is adept at handling large
datasets, characteristic of environmental sensor data. Spark's features are
leveraged as follows:

\begin{enumerate}
    \item Spark's architecture is uniquely designed to manage the fluctuating volume and
          nature of IoT sensor data. This scalability is critical for ensuring the
          efficient processing of data in real-time, which is essential for timely AQI
          calculation.

    \item The distributed nature of Spark facilitates the parallel processing of tasks
          across multiple nodes. This capability is vital for the simultaneous processing
          of AQI calculations from multiple sensors, enhancing the efficiency and speed
          of data computation.

    \item This methodical approach, leveraging Spark's scalability and distributed
          processing capabilities, ensures that the project achieves prompt and accurate
          AQI values, which are crucial for real-time air quality monitoring and
          assessment.
\end{enumerate}

\subsection{Data Storing}

\subsubsection{Database Choice}
The difficulty with this project lies in the efficient management of data
storage. Each sensor can measure a variety of data, making the structure of a
single relational database too complex and inefficient. This complexity
increases as the volume of data increases, requiring the creation of multiple
linked tables. This structural challenge poses a performance problem,
particularly when it comes to processing and analysing large quantities of data
in real time, a crucial aspect for applications such as air quality or
environmental monitoring.

Faced with these challenges, time series databases (TSDBs) are emerging as an
optimal solution. Unlike traditional relational databases, TSDBs are
specifically designed to manage time-series data, such as that from IoT
sensors. They offer better performance in tracking, monitoring and aggregating
data over time. In addition, TSDBs such as Amazon Timestream stand out for
their ability to ingest data quickly and process large volumes efficiently,
while ensuring regular updates and in-depth analysis, essential aspects for
applications such as Air Quality Index analysis.

Amazon Timestream is a good choice for this project. As a cloud-native
time-series database, it offers superior time-series management capabilities
tailored to IoT sensor data. What sets Amazon Timestream apart is the speed
with which it ingests data and its efficiency in processing large volumes of
data, enabling regular updates and in-depth analysis. Its ability to adapt to
changing workloads ensures consistent performance, a major advantage for
real-time data management. What's more, its advanced security features meet
strict standards of confidentiality and data sovereignty, an essential

\subsubsection{Storing Script}
The data storing process represents the final phase in the IoT data collecting
and processing pipeline. Once the Air Quality Index (AQI) has been calculated,
the data must be stored efficiently and securely to allow subsequent analysis
and real-time consultation. The \texttt{storing.py} script (see
Appendix~ref{appendix:storing} for detailed code) is designed to interact with
the Amazon Timestream database.

The \texttt{keepOnlyUpdatedRows} function is responsible for checking what data
is already in Timestream and keeping only the newly updated values. This
process begins by querying Timestream to retrieve the latest data timestamp for
each sensor using the `boto3` client. The data is then filtered to exclude
records that do not reflect new measurements, optimising storage space and
database performance.

Once this filtering is complete, the \texttt{writeToTimestream} function takes
over. It transforms each partition of the Spark DataFrame into a series of
structured records, containing the dimensions and measurements corresponding to
each sensor. These records are then written to Timestream in batches. The
process is carefully managed to ensure that records are written atomically and
consistently, with robust exception handling to deal with any rejected records.

In conclusion, the integration of Spark with Amazon Timestream offers a
powerful solution for storing environmental sensor data. Spark's ability to
process and filter data in a distributed manner aligns perfectly with
Timestream's high availability and auto-scaling capabilities, ensuring a
resilient, high-performance data infrastructure. This arrangement enables
reliable data storage and rapid retrieval, which are essential for continuous
monitoring of air quality and rapid responses to environmental issues.

\newpage
\subsection{Pipeline Implementation on AWS}
Setting up an IoT data processing pipeline on Amazon Web Services (AWS)
involves several key steps, from configuring the EC2 instance to running and
managing the pipeline. The process is described below:

\begin{enumerate}
    \item \textbf{EC2 Instance Configuration:} An EC2 t4g.small instance with Ubuntu~\ref{fig:ec2-instance-configuration} has been configured to host the data collecting, processing \& storing pipeline. This type of instance was chosen for its balance between performance and energy efficiency. These instances, based on ARM architecture, are ideal for light to moderate workloads, which correspond to the requirements of continuous data processing. In addition, Apache Spark has been installed on this instance to process the data collected.
    \item \textbf{Setting Up Automated Services:} Two systemd services, which are activated each time the instance is started, were then set up to automate the following tasks:
          \begin{itemize}
              \item \textbf{get\_iam\_credentials.service:} This service (See Appendix~\ref{appendix:get-iam-credentials-service}) executes \texttt{get\_iam\_credentials.sh} (See Appendix~\ref{appendix:get-iam-credentials}). This script retrieves the instance's IAM credentials, enabling secure integration with other AWS services.
              \item \textbf{spark\_python\_job.service:} This service (See Appendix~\ref{appendix:spark-python-job-service}) launches \texttt{start\_spark\_job.sh} (See Appendix~\ref{appendix:start-spark-job}), which starts the script responsible for the collection, processing and storage of IoT data (See Appendix~\ref{appendix:main}).
          \end{itemize}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/t4g-small.png}
    \caption{EC2 Instance Configuration Screenshot}\label{fig:ec2-instance-configuration}
\end{figure}

\newpage
\section{Data Distributing}

\subsection{Overview of the second pipeline architecture}
The second pipeline in this project is dedicated to the effective distribution
of processed data. This pipeline is comprised of several key components, each
integral to the efficient dissemination and monitoring of data.

\begin{enumerate}
    \item \textbf{Internet Gateway}: \\
          The user's interaction with the system begins at the secure Internet gateway. This gateway serves as the primary access point to the system, directing users to the Grafana dashboards through a specified URL. This approach ensures secure and controlled access to the air quality data visualisations.

    \item \textbf{Load Balancer}: \\
          At the heart of the data distribution pipeline is the Load Balancer. This component is tasked with evenly distributing incoming traffic across multiple EC2 instances. The Load Balancer plays a crucial role in balancing the workload and optimising resource utilisation, ensuring that no single instance is overwhelmed, thus maintaining smooth and efficient data flow.

    \item \textbf{Grafana Dashboard Access}: \\
          For data visualisation, the Grafana dashboard is utilised. Hosted on a Load Balancer, the dashboard presents the data stored in the Amazon Timestream database. It offers real-time visualisation of air quality metrics, enabling users to interact with and analyse the data in a user-friendly manner. This real-time access is critical for timely decision-making and assessment of air quality trends.
    \item \textbf{CloudWatch Monitoring}: \\
          Finally, the AWS CloudWatch service continuously monitors the overall system performance. It specifically tracks the CPU utilization of all EC2 instances within the Auto Scaling Group. Based on the aggregated CPU usage data, CloudWatch dynamically adjusts the system's resources. This adaptive scaling is essential for maintaining the balance between system efficiency and cost-effectiveness, ensuring the infrastructure responds appropriately to varying loads.
\end{enumerate}

For a comprehensive visual overview of this pipeline, please refer to
Figure~\ref{fig:data-distributing-pipeline}, which depicts the Data
Distributing Pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/cloud-computing-clients.png}
    \caption{Data Distributing Pipeline Diagram}\label{fig:data-distributing-pipeline}
\end{figure}

\newpage
\subsection{Auto Scaling Group Configuration}

The Auto Scaling Group plays a crucial role in dynamically managing the EC2
instances of the Data Distributing Pipeline. This section outlines the
configurations for the Auto Scaling Group and its associated components. The
figure below~\ref{fig:auto-scaling-group-pipeline} illustrates the Auto Scaling
Group Pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/grafana.png}
    \caption{Auto Scaling Group Pipeline Diagram}\label{fig:auto-scaling-group-pipeline}
\end{figure}

\newpage
\subsubsection{AMI Configuration}
An Amazon Machine Image (AMI) serves as a blueprint for launching an EC2
instance, containing the necessary software configuration. For this project, a
custom AMI was created using the Amazon Linux 2 operating system, specifically
tailored to host the Grafana server for efficient data visualisation.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/ami.png}
    \caption{AMI Settings Screenshot}\label{fig:ami-settings}
\end{figure}

\newpage
\subsubsection{Launch Template Configuration}
The launch template defines the specifications for deploying EC2 instances
within the Auto Scaling Group. It incorporates the custom AMI and delineates
instance type, security group, key pair, and user data. This template deploys
t2.micro instances, which are ideal for light workloads, such as hosting the
Grafana server. The template also specifies the security group, which allows
inbound traffic on port 3000, enabling access to the Grafana dashboard.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/launch-template.png}
    \caption{Launch Template Settings Screenshot}\label{fig:launch-template-settings}
\end{figure}

\newpage
\subsubsection{Auto Scaling Group Configuration}
The Auto Scaling Group is configured to automatically adjust the number of EC2
instances in response to the changing demand. It scales between 1 and 5
instances based on CPU utilisation, enhancing the system's responsiveness and
efficiency. The group is also linked to the specified target group and load
balancer, facilitating effective traffic distribution.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/auto-scaling-group.png}
    \caption{Auto Scaling Group Settings Screenshot}\label{fig:auto-scaling-group-settings}
\end{figure}

The Auto Scaling Group employs the following policies for scaling:
\begin{itemize}
    \item \textbf{Scale-out Policy:} Activates when average CPU utilisation exceeds 60\% for 3 minutes, adding an instance to the group.
    \item \textbf{Scale-in Policy:} Triggers when average CPU utilisation drops below 42\% for 15 minutes, removing an instance.
    \item \textbf{Cooldown Period:} Set at 150 seconds to maintain stability by preventing frequent scaling.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/scaling-policy.png}
    \caption{Scaling Policy Settings Screenshot}\label{fig:scaling-policy-settings}
\end{figure}

\subsubsection{Target Group Configuration}

A target group was created to distribute traffic between the EC2 instances of
the Auto Scaling Group.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/target-group.png}
    \caption{Target Group Settings Screenshot}\label{fig:target-group-settings}
\end{figure}

\newpage
\subsubsection{Load Balancer Configuration}
A load balancer was created to distribute traffic between the EC2 instances of
the Auto Scaling Group. The load balancer is configured to use the target group
and to listen on port 80. This is useful for ensuring that the Grafana
dashboard is accessible to users through a user-friendly URL.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/load-balancer.png}
    \caption{Load Balancer Settings Screenshot}\label{fig:load-balancer-settings}
\end{figure}

\newpage
\chapter{Results \& Discussion}

\section{Results}
\subsection{Accessing the Data - Grafana Dashboard}

The Grafana dashboard, a vital tool in air quality monitoring, is accessible at
the following URL:
\href{http://grafana-1777174802.us-east-1.elb.amazonaws.com/d/f8742187-f440-4ee8-96cc-bad5af8edef1/air-quality-monitoring}{Air
    Quality Monitoring Dashboard}.

The Grafana dashboard provides an interactive interface enabling users to
analyse air quality data. Users can customise the time interval, measurement
value and location ID. The dashboard consists of five main panels, each
providing unique information::
\begin{enumerate}
    \item \textbf{Measure Value Evolution:} This panel shows the evolution of the selected measure value in the selected location over the selected time range. This panel can be seen in Figure~\ref{fig:grafana-main-panel-1}.
    \item \textbf{Max AQI:} This panel shows the maximum AQI value in the selected location over the selected time range. This panel can be seen in Figure~\ref{fig:grafana-main-panel-1}.
    \item \textbf{Mean AQI:} This panel shows the mean AQI value in the selected location over the selected time range. This panel can be seen in Figure~\ref{fig:grafana-main-panel-1}.
    \item \textbf{AQI Map:} This panel shows the AQI value of each location on a map. This panel can be seen in Figure~\ref{fig:grafana-main-panel-2}.
    \item \textbf{IoT Data:} This panel shows all the data collected by the IoT sensors related to the selected measure value and time range. This panel can be seen in Figure~\ref{fig:grafana-main-panel-2}.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/grafana-1.png}
    \caption{Grafana Dashboard: Measure Value Evolution, Max \& Mean AQI}\label{fig:grafana-main-panel-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/grafana-2.png}
    \caption{Grafana Dashboard: AQI Map \& IoT Data}\label{fig:grafana-main-panel-2}
\end{figure}

\newpage
As shown in Figure~\ref{fig:grafana-aqi-map-panel}, the AQI Map panel displays
the AQI value of each location on a map. The AQI value is represented by a
colour, depending to the range it belongs to. The AQI value is also displayed
when hovering over a location.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/grafana-4.png}
    \caption{Grafana Dashboard AQI Map Panel}\label{fig:grafana-aqi-map-panel}
\end{figure}

As required, the Grafana dashboard enables users to customise the time
interval, as shown in Figure~\ref{fig:grafana-iot-data-panel-time-range}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/time-range.png}
    \caption{Grafana Dashboard IoT Data Panel - Time Range Choice}\label{fig:grafana-iot-data-panel-time-range}
\end{figure}

\newpage
It is also possible to choose the measure value to display in the IoT Data panel, as shown in Figure~\ref{fig:grafana-iot-data-panel-measure-value}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/measure-value.png}
    \caption{Grafana Dashboard IoT Data Panel - Measure Value Choice}\label{fig:grafana-iot-data-panel-measure-value}
\end{figure}

Moreove, it is possible to apply a filter to the data displayed in the IoT Data
panel. This can be done by clicking on the funnel icon, as shown in
Figure~\ref{fig:grafana-iot-data-panel-filter}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/filter.png}
    \caption{Grafana Dashboard IoT Data Panel - Apply a Filter}\label{fig:grafana-iot-data-panel-filter}
\end{figure}

\newpage
In addition, it is possible to export the data displayed in the IoT Data panel to a CSV file. This can be done by clicking on the \texttt{Download CSV} button, as shown in Figure~\ref{fig:grafana-iot-data-panel-export}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/export-2.png}
    \caption{Grafana Dashboard IoT Data Panel - Export Values}\label{fig:grafana-iot-data-panel-export}
\end{figure}

\newpage
\subsection{Load Test}
In order to test the scalability of the system, a load test was performed using
the Artillery load testing tool. The test consisted in simulating an increasing
number of customers consuming the grafana dashboards simultaneously, to
demonstrate the elasticity of the solution developed. The results of the test
are shown in Figure~\ref{fig:load-test-result}.

\subsubsection{Artillery Load Test}
The load test was performed using the Artillery load testing script available
in Appendix~\ref{appendix:artillery-load-test}. Here are the steps of the load
test:
\begin{enumerate}
    \item \textbf{Stage 1:} 1 new user is added every second for 50 seconds.
    \item \textbf{Pause:} No new users are added for 30 seconds.
    \item \textbf{Stage 2:} 1 new user is added every second for 60 seconds.
    \item \textbf{Pause:} No new users are added for 60 seconds.
    \item \textbf{Stage 3:} 1 new user is added every second for 60 seconds.
    \item \textbf{Pause:} No new users are added for 30 seconds.
    \item \textbf{Stage 4:} 2 new users are added every second for 50 seconds.
    \item \textbf{Pause:} No new users are added for 60 seconds.
    \item \textbf{Stage 5:} 2 new users are added every second for 60 seconds.
    \item \textbf{Pause:} No new users are added for 60 seconds.
    \item \textbf{Stage 6:} 2 new users are added every second for 80 seconds.
\end{enumerate}
Each user is configured to access 2 endpoints of the Grafana dashboard 100 times. Besides, this test was performed several times to ensure the reliability of the results.

\subsubsection{Auto Scaling Group Behaviour Monitoring}
The Auto Scaling Group behaviour was monitored during the load test thanks to a
Python script available in Appendix~\ref{appendix:monitoring}. The monitoring
data is available in the \texttt{metrics.csv} file in the \texttt{test} folder.
Below~\ref{fig:load-test-result} is a graph showing the CPU utilisation of the
Auto Scaling Group during the load test.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/autoscaling-test.png}
    \caption{Load Test Result}\label{fig:load-test-result}
\end{figure}

\newpage
\section{Data Security and Sovereignty Considerations}
Data security and sovereignty are critical in the management of environmental
sensor data. Recent research trends in IoT security highlight the evolving
landscape of threats and protections in this field
\cite{CurrentResearchIoTSecurity2020}.

\subsection{Data Security}
In the realm of IoT, ensuring data security involves advanced encryption,
robust access control, and maintaining data integrity. The systematic mapping
study by \citeauthor{CurrentResearchIoTSecurity2020} provides insights into
current methodologies and research directions in IoT security, relevant to
environmental sensor data.

\subsection{Data Sovereignty}
Data sovereignty is influenced by varying international legislations, such as
GDPR, affecting data storage and cross-border data transfer. The study by
\citeauthor{CurrentResearchIoTSecurity2020} underscores the importance of
understanding these legal complexities in IoT environments.

\subsection{Specific Fields for Further Examination}
Areas needing heightened attention include personal data protection,
sensitivity of environmental data, and metadata security, as indicated by
recent IoT security research trends \cite{CurrentResearchIoTSecurity2020}.

Adhering to the latest security trends and regulatory frameworks is vital for
the ethical management of environmental sensor data. The evolving research in
IoT security serves as a guide for future developments in this field
\cite{CurrentResearchIoTSecurity2020}.

\newpage
\chapter{Conclusion}

In conclusion, this study presents a significant advancement in air quality
monitoring through IoT technologies, demonstrating the effectiveness of the
Grafana dashboard in data visualisation and analysis. The successful load test
indicates the system's scalability, catering to simultaneous user access.
Addressing data security and sovereignty, the project highlights the importance
of robust data management practices in IoT environments. While acknowledging
certain limitations, this research lays a foundation for future explorations in
IoT security and data handling. Overall, it contributes valuable insights to
the field of environmental monitoring and data security, underscoring the
potential of IoT in addressing critical ecological issues.

\bibliographystyle{CranfieldNumbered}
\bibliography{CUCitations}

\appendix
\chapter{Documentation}

\begin{subappendices}
    \section{Project tree}
    \begin{lstlisting}[breaklines=true, basicstyle=\small]
    lib/
        collecting.py
        processing.py
        storing.py
    scripts/
        get_iam_credentials.sh
        start_spark_job.sh
    services/
        get_iam_credentials.service
        spark_python_job.service
        grafana_server.service
    test/
        artillery_load_test.yml
        monitoring.py
        metrics.csv
        results.json
        visualisation_load_test.ipynb
    main.py
    README.md
    requirements.txt
    \end{lstlisting}

    \section{Getting Started}
    To run the program, follow these steps:
    \begin{enumerate}
        \itemindent=17.87pt
        \item Create a virtual environment using \texttt{python3 -m venv venv}.
        \item Activate the virtual environment using \texttt{source venv/bin/activate}.
        \item Install the required dependencies using \texttt{pip3 install -r
                  requirements.txt}.
        \item Run the program using \texttt{python3 main.py}.
        \item Visualise the results using \texttt{visualisation.ipynb} (Jupyter Notebook).
    \end{enumerate}

    \section{Detailed Features of Functions}
    \begin{description}
        \item \texttt{collecting.py}
              \begin{itemize}
                  \item \texttt{fetch\_sensors\_data(sparkSession)}: Function to ingest the latest data from the sensors and returns it as a Spark DataFrame.
              \end{itemize}

        \item \texttt{processing.py}
              \begin{itemize}
                  \item \texttt{get\_aqi\_value\_p25(value)}: Function for calculating the AQI value for PM2.5.
                  \item \texttt{get\_aqi\_value\_p10(value)}: Function for calculating the AQI value for PM10.
                  \item \texttt{computeAQI(df)}: Function for calculating the AQI value for each particulate matter sensor and returning the DataFrame with the AQI column.
              \end{itemize}

        \item \texttt{storing.py}
              \begin{itemize}
                  \item \texttt{keepOnlyUpdatedRows(database\_name, table\_name, df)}: Function for keeping only the rows that have been updated in the DataFrame.
                  \item \texttt{\_print\_rejected\_records\_exceptions(err)}: Internal function for printing the rejected records exceptions.
                  \item \texttt{write\_records(database\_name, table\_name, client, records)}: Internal function for writing a batch of records to the Timestream database.
                  \item \texttt{writeToTimestream(database\_name, table\_name, partionned\_df)}: Function for writing the DataFrame to the Timestream database.
              \end{itemize}
    \end{description}
\end{subappendices}

\chapter{Source Codes}
\begin{subappendices}
    \section{Ingestion, Processing \& Storing Pipeline Source Code}\label{appendix:main}
    \lstinputlisting[style=pythonstyle]{../main.py}

    \newpage
    \section{Data Collecting Source Code}\label{appendix:collecting}
    \lstinputlisting[style=pythonstyle]{../lib/collecting.py}

    \newpage
    \section{Data Processing Source Code}\label{appendix:processing}
    \lstinputlisting[style=pythonstyle]{../lib/processing.py}

    \newpage
    \section{Data Storing Source Code}\label{appendix:storing}
    \lstinputlisting[style=pythonstyle]{../lib/storing.py}

    \newpage
    \section{Scripts \& Services Source Codes }
    \subsection{Scripts}

    Script used by the \texttt{get\_iam\_credentials} service to retrieve the IAM
    credentials from the metadata server.\label{appendix:get-iam-credentials}
    \lstinputlisting[style=bashstyle]{../scripts/get_iam_credentials.sh}

    Script used by the \texttt{spark\_python\_job} service to run the Python Spark
    job.\label{appendix:start-spark-job}
    \lstinputlisting[style=bashstyle]{../scripts/start_spark_job.sh}

    \newpage
    \subsection{Services}

    \subsubsection{Get IAM Credentials Service}
    Service used by the Ubuntu EC2 instance to retrieve the IAM credentials from
    the metadata server.\label{appendix:get-iam-credentials-service}
    \lstinputlisting[style=servicestyle]{../services/get_iam_credentials.service}

    \subsubsection{Spark Python Job Service}
    Service used by the Ubuntu EC2 instance to run the Python Spark job (Data
    Collecting, Processing and Storing).\label{appendix:spark-python-job-service}
    \lstinputlisting[style=servicestyle]{../services/spark_python_job.service}

    \newpage
    \subsubsection{Grafana Server Service}
    Service used by the Linux EC2 instances to run the Grafana server (Data
    Distributing).\label{appendix:grafana-server-service}
    \lstinputlisting[style=servicestyle]{../services/grafana_server.service}

    \newpage
    \section{Load Test Source Codes }

    \subsection{Artillery Load Test Configuration File}\label{appendix:artillery-load-test}
    \lstinputlisting[style=ymlstyle]{../test/artillery_load_test.yml}

    \newpage
    \subsection{Instances Monitoring Script}\label{appendix:monitoring}
    \lstinputlisting[style=pythonstyle]{../test/monitoring.py}
\end{subappendices}

\end{document}

